{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d17c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "input_jsonl_path = \"//teamspace/studios/this_studio/Inferences/test2.jsonl\"  \n",
    "label_map = {-1.0: 0, 0.0: 1, 1.0: 2}\n",
    "label_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# ---------- LOAD DATA ----------\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with open(input_jsonl_path, 'r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            aspect = item[\"aspect\"]\n",
    "            gt_raw = item[\"sentiment_dict\"].get(aspect, None)\n",
    "            pred = item[\"predicted_sentiment\"].get(aspect, None)\n",
    "\n",
    "            if gt_raw is None or pred is None:\n",
    "                continue\n",
    "\n",
    "            gt_mapped = label_map.get(gt_raw)\n",
    "            if gt_mapped is None:\n",
    "                continue\n",
    "\n",
    "            y_true.append(gt_mapped)\n",
    "            y_pred.append(pred)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping line: {e}\")\n",
    "            continue\n",
    "\n",
    "# ---------- METRICS ----------\n",
    "report = classification_report(y_true, y_pred, target_names=label_names, digits=4, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(\"Classification Report (weighted):\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_names, digits=4))\n",
    "print(\"Weighted Precision:\", round(precision, 4))\n",
    "print(\"Weighted Recall:   \", round(recall, 4))\n",
    "print(\"Weighted F1 Score: \", round(f1, 4))\n",
    "\n",
    "# ---------- VISUALIZATION ----------\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "pale_yellow_cmap = LinearSegmentedColormap.from_list(\"pale_yellow_shades\", [\"#FFF9C4\", \"#FFEB3B\", \"#FFEB8C\", \"#FFCC00\"])\n",
    "\n",
    "#Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=pale_yellow_cmap, \n",
    "            xticklabels=label_names, yticklabels=label_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Class Distribution \n",
    "counts_true = Counter(y_true)\n",
    "counts_pred = Counter(y_pred)\n",
    "\n",
    "labels = label_names\n",
    "true_counts = [counts_true.get(i, 0) for i in range(3)]\n",
    "pred_counts = [counts_pred.get(i, 0) for i in range(3)]\n",
    "\n",
    "x = range(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(x, true_counts, width, label='Ground truth', color='#FFD700')  # Yellow\n",
    "plt.bar([p + width for p in x], pred_counts, width, label='Predicted', color='#FF8C00')  # Orange\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class distribution: Ground truth vs Predicted')\n",
    "plt.xticks([p + width / 2 for p in x], labels)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
