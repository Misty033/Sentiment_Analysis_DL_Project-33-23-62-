{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1b7ccc",
   "metadata": {},
   "source": [
    "#### SOTA_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertModel\n",
    "\n",
    "#Data\n",
    "train_df = pd.read_parquet(\"train.parquet\")\n",
    "test_df = pd.read_parquet(\"test2.parquet\")\n",
    "\n",
    "def extract_valid_samples(df):\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row[\"sentence\"]\n",
    "        aspect = row[\"aspect\"]\n",
    "        sentiment_dict = row[\"sentiment_dict\"]\n",
    "        sentiment = sentiment_dict.get(aspect, None)\n",
    "        if sentiment is not None:\n",
    "            input_text = f\"aspect: {aspect} sentence: {sentence}\"\n",
    "            data.append((input_text, sentiment))\n",
    "    return pd.DataFrame(data, columns=[\"text\", \"label\"])\n",
    "\n",
    "train_data = extract_valid_samples(train_df)\n",
    "test_data = extract_valid_samples(test_df)\n",
    "train_data.to_csv(\"train_processed.csv\", index=False)\n",
    "test_data.to_csv(\"test_processed.csv\", index=False)\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "\n",
    "\n",
    "#Preprocessing\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "test_df = pd.read_csv(\"test_processed.csv\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "label_map = {-1: 0, 0: 1, 1: 2}\n",
    "train_df[\"label\"] = train_df[\"label\"].map(label_map)\n",
    "test_df[\"label\"] = test_df[\"label\"].map(label_map)\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(self.texts[idx], truncation=True, padding='max_length',\n",
    "                             max_length=self.max_len, return_tensors='pt')\n",
    "        item = {key: val.squeeze(0) for key, val in enc.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "\n",
    "#Bert_Base_Model\n",
    "class Bert_Base(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\", num_labels=num_classes)\n",
    "        print(\"Loaded Bert_Base\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        output = self.bert(input_ids=input_ids,\n",
    "                           attention_mask=attention_mask,\n",
    "                           token_type_ids=token_type_ids,\n",
    "                           labels=labels)\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "\n",
    "#Bert_LSTM_Model\n",
    "class Bert_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "        self.lstm = nn.LSTM(input_size=768, hidden_size=128, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        print(\"Loaded Bert_LSTM\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        x = torch.stack([hidden_states[i][:, 0] for i in range(1, len(hidden_states))], dim=1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.dropout(lstm_out[:, -1, :])\n",
    "        logits = self.fc(out)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "\n",
    "#Bert_Attention_Model\n",
    "class Bert_Attention(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "        self.q = nn.Parameter(torch.randn(1, 768))\n",
    "        self.w_h = nn.Parameter(torch.randn(768, 256))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "        print(\"Loaded Bert_Attention\")\n",
    "\n",
    "    def attention(self, h):\n",
    "        v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n",
    "        v = torch.softmax(v, dim=-1)\n",
    "        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n",
    "        v = torch.matmul(self.w_h.T, v_temp).squeeze(2)\n",
    "        return v\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        x = torch.stack([hidden_states[i][:, 0] for i in range(len(hidden_states) - 12, len(hidden_states))], dim=1)\n",
    "        attn_out = self.attention(x)\n",
    "        out = self.dropout(attn_out)\n",
    "        logits = self.fc(out)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "#Training and Evaluation\n",
    "def train_eval_model(model, train_loader, test_loader, epochs=3):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            loss, _ = model(**inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            _, logits = model(**inputs)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(inputs['labels'].cpu().tolist())\n",
    "\n",
    "    inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    y_true = [inverse_label_map[l] for l in all_labels]\n",
    "    y_pred = [inverse_label_map[p] for p in all_preds]\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "    \n",
    "\n",
    "train_dataset = ABSADataset(train_df[\"text\"].tolist(), train_df[\"label\"].tolist(), tokenizer)\n",
    "test_dataset = ABSADataset(test_df[\"text\"].tolist(), test_df[\"label\"].tolist(), tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "model = Bert_Base(num_classes=3)       \n",
    "train_eval_model(model, train_loader, test_loader)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = ABSADataset(train_df[\"text\"].tolist(), train_df[\"label\"].tolist(), tokenizer)\n",
    "test_dataset = ABSADataset(test_df[\"text\"].tolist(), test_df[\"label\"].tolist(), tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "model = Bert_LSTM(num_classes=3)       \n",
    "train_eval_model(model, train_loader, test_loader)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = ABSADataset(train_df[\"text\"].tolist(), train_df[\"label\"].tolist(), tokenizer)\n",
    "test_dataset = ABSADataset(test_df[\"text\"].tolist(), test_df[\"label\"].tolist(), tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "model = Bert_Attention(num_classes=3)        \n",
    "train_eval_model(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b1d6f",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a146e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['BERT-base', 'BERT-LSTM', 'BERT-Attention', 'BERT-MultiheadAttention']\n",
    "f1_scores = [0.464, 0.490, 0.582, 0.8529]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(models, f1_scores, color=['skyblue', 'lightgreen', 'orange', 'purple'])\n",
    "\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f\"{score:.3f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Weighted Avg F1 Score')\n",
    "plt.title('Weighted F1 Comparison Across BERT-Based SOTA Models')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
